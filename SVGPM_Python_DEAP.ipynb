{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVGPM_Python_DEAP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPym9IYmAmvektpYfps5Hv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spozi/svgpm/blob/main/SVGPM_Python_DEAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_ZEI1ZknHnT",
        "outputId": "03c0c9b7-578b-4866-ebbe-3f059af5e9c3"
      },
      "source": [
        "!pip install -U deap imbalanced-learn scikit-learn-intelex"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deap in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: scikit-learn-intelex in /usr/local/lib/python3.7/dist-packages (2021.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deap) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (0.24.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (2.2.0)\n",
            "Requirement already satisfied: daal4py==2021.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-intelex) (2021.3.0)\n",
            "Requirement already satisfied: daal==2021.3.0 in /usr/local/lib/python3.7/dist-packages (from daal4py==2021.3.0->scikit-learn-intelex) (2021.3.0)\n",
            "Requirement already satisfied: tbb==2021.* in /usr/local/lib/python3.7/dist-packages (from daal==2021.3.0->daal4py==2021.3.0->scikit-learn-intelex) (2021.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irgcpLpFBF-3",
        "outputId": "32dab90a-d750-4dcb-de69-6bbbf251d363"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import site\n",
        "from sklearn.model_selection import train_test_split\n",
        "sys.path.append(os.path.join(os.path.dirname(site.getsitepackages()[0]), \"site-packages\"))\n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASNSiD1zsUJ8"
      },
      "source": [
        "#@title SVGPM Configuration\n",
        "#@markdown ---\n",
        "#@markdown ### Enter a file path:\n",
        "train_file_path = \"/content/train_ecoli.csv\" #@param {type:\"string\"}\n",
        "test_file_path = \"/content/test_ecoli.csv\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Enter SVGPM Parameters:\n",
        "population_size = 200 #@param {type:\"slider\", min:0, max:1000, step:2}\n",
        "number_of_generation = 200 #@param {type:\"slider\", min:0, max:1000, step:2}\n",
        "\n",
        "#@markdown ### Enter 2^C and 2^Gamma Parameter:\n",
        "C = 0 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
        "gamma = 0 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
        "#@markdown ---\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjaiCtAhnPHP"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "#Train and test file\n",
        "train_file = train_file_path\n",
        "test_file = test_file_path\n",
        "\n",
        "train_data = np.loadtxt(train_file, delimiter=\",\", skiprows=1)\n",
        "test_data = np.loadtxt(test_file, delimiter=\",\", skiprows=1)\n",
        "\n",
        "X_train = train_data[:, 1:]\n",
        "X_test = test_data[:, 1:]\n",
        "\n",
        "#Scale each feature to 0 and 1\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#y_train\n",
        "y_train = train_data[:, 0].astype(int)\n",
        "y_test = test_data[:, 0].astype(int)\n",
        "\n",
        "#Compute class weight\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "pos_index = 0\n",
        "neg_index = 0\n",
        "\n",
        "class_weight = {}\n",
        "if counts[0] < counts[1]:\n",
        "  class_weight = {unique[0]: counts[1], unique[1]: counts[0]}\n",
        "else:\n",
        "  class_weight = {unique[1]: counts[0], unique[0]: counts[1]}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6GNqTO0nnRv",
        "outputId": "7397ce17-1eba-4fd2-9619-8558e5ccca71"
      },
      "source": [
        "print(unique, counts, class_weight)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1  1] [201  23] {1: 201, -1: 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n4G13-SRMp2"
      },
      "source": [
        "Version 2 GP Feature Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UpoPcmQMMda"
      },
      "source": [
        "import random\n",
        "import operator\n",
        "import math\n",
        "import statistics\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from deap import algorithms\n",
        "from deap import base\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "from deap import gp\n",
        "\n",
        "# Define new functions\n",
        "def protectedDiv(left, right):\n",
        "    try:\n",
        "        return left / right\n",
        "    except ZeroDivisionError:\n",
        "        return 1\n",
        "\n",
        "nFeatures = X_train.data.shape[1]\n",
        "pset = gp.PrimitiveSet(\"MAIN\", nFeatures) \n",
        "pset.addPrimitive(operator.add, 2)\n",
        "pset.addPrimitive(operator.sub, 2)\n",
        "pset.addPrimitive(operator.mul, 2)\n",
        "pset.addPrimitive(protectedDiv, 2)\n",
        "pset.addPrimitive(operator.neg, 1)\n",
        "pset.addPrimitive(math.erfc, 1)\n",
        "pset.addPrimitive(math.gamma, 1)\n",
        "pset.addPrimitive(math.erf, 1)\n",
        "pset.addPrimitive(math.exp, 1)\n",
        "pset.addPrimitive(math.sqrt, 1)\n",
        "pset.addPrimitive(math.cos, 1)\n",
        "pset.addPrimitive(math.sin, 1)\n",
        "pset.addEphemeralConstant(\"rand\", lambda: round(random.uniform(0.1, 1.0), 4))\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0, ))\n",
        "creator.create(\"Tree\", gp.PrimitiveTree)\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"main_expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=5)\n",
        "toolbox.register('MAIN', tools.initIterate, creator.Tree, toolbox.main_expr)\n",
        "\n",
        "func_cycle = [toolbox.MAIN]\n",
        "\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual, func_cycle)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71GGjz4mQ9SS"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "from multiprocessing import Pool\n",
        "\n",
        "from time import time\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def evalSymbReg(score):\n",
        "  return score, \n",
        "\n",
        "def evalSymbRegPop(population):\n",
        "  # Evaluate each individual in population\n",
        "  #1. Compute the expression of every individual\n",
        "  list_vecs = []\n",
        "  # start = time()\n",
        "  for individual in population:\n",
        "    #Evaluating expression on each vector\n",
        "    func = toolbox.compile(expr=individual)\n",
        "    vec = []\n",
        "    for x in X_train: #Iterate every vector x (row) in data (matrix) X\n",
        "      try:\n",
        "        val = func(*x)\n",
        "        vec.append(val)\n",
        "      except:\n",
        "        vec.append(0)\n",
        "    list_vecs.append(vec)\n",
        "  end = time()\n",
        "  # expression_evaluation_time = end - start\n",
        "\n",
        "  #2. Convert list_vecs to numpy array\n",
        "  evaluated_X = np.array(list_vecs).T\n",
        "  evaluated_X = np.float32(evaluated_X)\n",
        "  evaluated_X = np.nan_to_num(evaluated_X, copy=True, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "  evaluated_X_train = np.hstack((X_train, evaluated_X)) #Merge x_train with evaluated_x\n",
        "\n",
        "  #3. Individual (feature) selection\n",
        "  # https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel\n",
        "\n",
        "  # start = time()\n",
        "  clf = ExtraTreesClassifier(n_estimators=50)\n",
        "  clf = clf.fit(evaluated_X_train, y_train)\n",
        "\n",
        "  #4. Extract features that at top threshold (get the 50 percentile)\n",
        "  q1 = np.percentile(clf.feature_importances_, 50)                                        #Get the top 50 percentile features\n",
        "  features = [True if val >= q1 else False for val in clf.feature_importances_.tolist()]  #Get the features indices\n",
        "  features = features[X_train.shape[1]:]                                                  #Extract GP Features from all features\n",
        "  X_train_new = evaluated_X[:, features]                                                  #Form a new training data with GP Features\n",
        "  # end = time()\n",
        "  # feature_evaluation_time = end - start\n",
        "\n",
        "  #5. Merge X_train with X_train_new row-wise\n",
        "  X_train_new = np.hstack((X_train, X_train_new))                                         #Merge back original training data with new GP based training data\n",
        "\n",
        "  #6. Use svc to get total nSV\n",
        "  # start = time()\n",
        "  clf_svc = SVC(C=2**C, gamma=2**gamma, class_weight=class_weight)      #Based on original paper \n",
        "  clf_svc.fit(X_train_new, y_train)                                     #98% of runtime is at here\n",
        "  # end = time()\n",
        "  # svm_training_time = end - start\n",
        "\n",
        "  # print(\"Expression evaluation: %.4f, Feature evaluation: %.4f, SVM training: %.4f\" % (expression_evaluation_time, feature_evaluation_time, svm_training_time))\n",
        "\n",
        "  y_pred = clf_svc.predict(X_train_new)\n",
        "\n",
        "  #7. Compute the score\n",
        "  nSV = clf_svc.support_vectors_.shape[0]\n",
        "  tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
        "  f1 = f1_score(y_train, y_pred)\n",
        "  accuracy = accuracy_score(y_train, y_pred)\n",
        "  specificity = tp / (tp + fn)\n",
        "  sensititivy = tn / (tn + fp)\n",
        "  gmean = math.sqrt(specificity * sensititivy)\n",
        "  fitness = min(gmean,accuracy)/nSV                    #This is single objective fitness function (based on original paper)\n",
        "  # fitness = (min(gmean,accuracy), min(gmean,accuracy)/nSV)            #This is new multi-objective fitness function\n",
        "\n",
        "  #8. Output the fitness value\n",
        "  ind_pop_fitness = []\n",
        "  for f in features:\n",
        "    if f is True:\n",
        "      ind_pop_fitness.append(fitness)     #If the feature is in the tree, set the fitness value to fitness\n",
        "    else:\n",
        "      ind_pop_fitness.append(0)       #If the feature is in the tree, set the fitness value to 0\n",
        "      # ind_pop_fitness.append((0,0))       #If the feature is in the tree, set the fitness value to 0\n",
        "  \n",
        "  return ind_pop_fitness\n",
        "\n",
        "psets = [pset]\n",
        "toolbox.register(\"compile\", gp.compileADF, psets=psets)\n",
        "toolbox.register('evaluate', evalSymbReg)\n",
        "toolbox.register('select', tools.selTournament, tournsize=3)\n",
        "toolbox.register('mate', gp.cxOnePoint)\n",
        "toolbox.register(\"expr\", gp.genFull, min_=1, max_=2)\n",
        "toolbox.register('mutate', gp.mutUniform, expr=toolbox.expr)\n",
        "\n",
        "toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=70))\n",
        "toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=70))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_7HXCBEMQmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6d6319-2a49-40ce-fbee-7c2fda754fa0"
      },
      "source": [
        "def main():\n",
        "  random.seed(1024)\n",
        "  ind = toolbox.individual()\n",
        "  \n",
        "  pop = toolbox.population(n=population_size)\n",
        "  hof = tools.HallOfFame(population_size)\n",
        "  stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "  stats.register(\"avg\", np.mean)\n",
        "  stats.register(\"std\", np.std)\n",
        "  stats.register(\"min\", np.min)\n",
        "  stats.register(\"max\", np.max)\n",
        "\n",
        "  logbook = tools.Logbook()\n",
        "  logbook.header = \"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"\n",
        "\n",
        "  CXPB, MUTPB, NGEN = 0.5, 0.2, number_of_generation\n",
        "\n",
        "  # # Evaluate the entire population\n",
        "  #1. Compute the metric on the set of individuals\n",
        "  ind_pop_fitness = evalSymbRegPop(pop)\n",
        "\n",
        "  #2. Then, determine the best individual using toolbox\n",
        "  for ind, fitness in zip(pop, ind_pop_fitness):\n",
        "    ind.fitness.values = toolbox.evaluate(fitness)\n",
        "\n",
        "  hof.update(pop)\n",
        "  record = stats.compile(pop)\n",
        "  logbook.record(gen=0, evals=len(pop), **record)\n",
        "  print(logbook.stream)\n",
        "\n",
        "  for g in range(1, NGEN):\n",
        "    # Select the offspring\n",
        "    offspring = toolbox.select(pop, len(pop))\n",
        "    # Clone the offspring\n",
        "    offspring = [toolbox.clone(ind) for ind in offspring]\n",
        "\n",
        "    # Apply crossover and mutation\n",
        "    for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
        "        for tree1, tree2 in zip(ind1, ind2):\n",
        "            if random.random() < CXPB:\n",
        "                toolbox.mate(tree1, tree2)\n",
        "                del ind1.fitness.values\n",
        "                del ind2.fitness.values\n",
        "\n",
        "    for ind in offspring:\n",
        "        for tree, pset in zip(ind, psets):\n",
        "            if random.random() < MUTPB:\n",
        "                toolbox.mutate(individual=tree, pset=pset)\n",
        "                del ind.fitness.values\n",
        "                        \n",
        "    # Evaluate the individuals with an invalid fitness\n",
        "    invalids = [ind for ind in offspring if not ind.fitness.valid]\n",
        "\n",
        "    #1. Compute the metric on the set of individuals\n",
        "    ind_pop_invalid_fitness = evalSymbRegPop(invalids)\n",
        "\n",
        "    #2. Then, determine the best individual using toolbox\n",
        "    for ind, fitness in zip(invalids, ind_pop_invalid_fitness):\n",
        "      ind.fitness.values = toolbox.evaluate(fitness)\n",
        "            \n",
        "    # Replacement of the population by the offspring\n",
        "    pop = offspring\n",
        "    hof.update(pop)\n",
        "    record = stats.compile(pop)\n",
        "    logbook.record(gen=g, evals=len(invalids), **record)\n",
        "    print(logbook.stream)\n",
        "  \n",
        "  # print('Best individual : ', hof[0][0], hof[0].fitness)\n",
        "  return pop, stats, hof\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pop, stats, hof = main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gen\tevals\tstd       \tmin\tavg       \tmax      \n",
            "0  \t200  \t0.00224215\t0  \t0.00224215\t0.0044843\n",
            "1  \t131  \t0.00218123\t0  \t0.00281588\t0.00452489\n",
            "2  \t113  \t0.0021251 \t0  \t0.00306242\t0.00456621\n",
            "3  \t104  \t0.00209691\t0  \t0.00343569\t0.00497512\n",
            "4  \t129  \t0.00233653\t0  \t0.00324932\t0.00502513\n",
            "5  \t97   \t0.00234461\t0  \t0.00378081\t0.00564972\n",
            "6  \t117  \t0.00323933\t0  \t0.0046027 \t0.00819067\n",
            "7  \t122  \t0.00316722\t0  \t0.00422219\t0.00819067\n",
            "8  \t110  \t0.00314625\t0  \t0.00433087\t0.00819067\n",
            "9  \t109  \t0.00332513\t0  \t0.00507447\t0.00819067\n",
            "10 \t113  \t0.00323973\t0  \t0.00508369\t0.00819067\n",
            "11 \t120  \t0.00344639\t0  \t0.00526093\t0.00819067\n",
            "12 \t120  \t0.0037748 \t0  \t0.00563234\t0.00865683\n",
            "13 \t124  \t0.00405378\t0  \t0.00582173\t0.00888871\n",
            "14 \t127  \t0.0105835 \t0  \t0.0114606 \t0.0259633 \n",
            "15 \t124  \t0.0103533 \t0  \t0.00933778\t0.0259633 \n",
            "16 \t123  \t0.00984018\t0  \t0.00927171\t0.0259633 \n",
            "17 \t118  \t0.00932491\t0  \t0.00965369\t0.0259633 \n",
            "18 \t119  \t0.00934027\t0  \t0.0102636 \t0.0259633 \n",
            "19 \t124  \t0.00932191\t0  \t0.0107072 \t0.0259633 \n",
            "20 \t112  \t0.00986138\t0  \t0.0109747 \t0.0259633 \n",
            "21 \t121  \t0.0098115 \t0  \t0.0108782 \t0.0259633 \n",
            "22 \t133  \t0.00907655\t0  \t0.0090608 \t0.0259633 \n",
            "23 \t107  \t0.00915333\t0  \t0.011464  \t0.0259633 \n",
            "24 \t120  \t0.00908254\t0  \t0.0106314 \t0.0259633 \n",
            "25 \t122  \t0.00816975\t0  \t0.0101273 \t0.0259633 \n",
            "26 \t112  \t0.00824372\t0  \t0.0114107 \t0.0259633 \n",
            "27 \t120  \t0.00825461\t0  \t0.0111558 \t0.0259633 \n",
            "28 \t124  \t0.00791216\t0  \t0.0109382 \t0.0259633 \n",
            "29 \t118  \t0.00797415\t0  \t0.0106683 \t0.0259633 \n",
            "30 \t120  \t0.0103849 \t0  \t0.013914  \t0.0259633 \n",
            "31 \t129  \t0.00997651\t0  \t0.0101081 \t0.0259633 \n",
            "32 \t123  \t0.00925514\t0  \t0.0103066 \t0.0259633 \n",
            "33 \t117  \t0.0095246 \t0  \t0.012178  \t0.0259633 \n",
            "34 \t114  \t0.00965874\t0  \t0.0131528 \t0.0259633 \n",
            "35 \t135  \t0.00882086\t0  \t0.0101791 \t0.0259633 \n",
            "36 \t115  \t0.00889924\t0  \t0.0119003 \t0.0259633 \n",
            "37 \t114  \t0.00937772\t0  \t0.0135167 \t0.0259633 \n",
            "38 \t123  \t0.00946167\t0  \t0.0127154 \t0.0259633 \n",
            "39 \t122  \t0.00906751\t0  \t0.0114774 \t0.0259633 \n",
            "40 \t112  \t0.00920828\t0  \t0.0130889 \t0.0259633 \n",
            "41 \t126  \t0.0092208 \t0  \t0.0120357 \t0.0259633 \n",
            "42 \t115  \t0.0087057 \t0  \t0.0117348 \t0.0259633 \n",
            "43 \t144  \t0.00813148\t0  \t0.00949143\t0.0259633 \n",
            "44 \t111  \t0.00752098\t0  \t0.00962591\t0.0259633 \n",
            "45 \t117  \t0.00799887\t0  \t0.0103874 \t0.0259633 \n",
            "46 \t134  \t0.00805199\t0  \t0.00932189\t0.0259633 \n",
            "47 \t112  \t0.00783427\t0  \t0.0093041 \t0.0259633 \n",
            "48 \t131  \t0.00767423\t0  \t0.00812571\t0.0259633 \n",
            "49 \t121  \t0.00807324\t0  \t0.00913417\t0.0259633 \n",
            "50 \t111  \t0.00810578\t0  \t0.00967771\t0.0259633 \n",
            "51 \t113  \t0.00846957\t0  \t0.0101684 \t0.0259633 \n",
            "52 \t123  \t0.00812056\t0  \t0.00939199\t0.0259633 \n",
            "53 \t133  \t0.00712249\t0  \t0.00765404\t0.0259633 \n",
            "54 \t129  \t0.00686178\t0  \t0.0077878 \t0.0259633 \n",
            "55 \t108  \t0.00747416\t0  \t0.00791922\t0.0259633 \n",
            "56 \t115  \t0.00759281\t0  \t0.00869699\t0.0259633 \n",
            "57 \t111  \t0.00792305\t0  \t0.00796885\t0.0259633 \n",
            "58 \t114  \t0.0072254 \t0  \t0.00810674\t0.0259633 \n",
            "59 \t129  \t0.00754546\t0  \t0.00817869\t0.0259633 \n",
            "60 \t108  \t0.00733954\t0  \t0.0088426 \t0.0259633 \n",
            "61 \t133  \t0.00716319\t0  \t0.00748163\t0.0259633 \n",
            "62 \t134  \t0.00632453\t0  \t0.00668113\t0.0259633 \n",
            "63 \t122  \t0.00638123\t0  \t0.00714707\t0.0259633 \n",
            "64 \t106  \t0.00647919\t0  \t0.00768499\t0.0259633 \n",
            "65 \t121  \t0.00757275\t0  \t0.00817575\t0.0259633 \n",
            "66 \t125  \t0.00723838\t0  \t0.00775562\t0.0259633 \n",
            "67 \t117  \t0.00808734\t0  \t0.00881215\t0.0259633 \n",
            "68 \t114  \t0.00816383\t0  \t0.00835525\t0.0259633 \n",
            "69 \t118  \t0.00795265\t0  \t0.00849277\t0.0259633 \n",
            "70 \t124  \t0.00824626\t0  \t0.00974006\t0.0259633 \n",
            "71 \t124  \t0.0108225 \t0  \t0.0137814 \t0.0259633 \n",
            "72 \t119  \t0.00992683\t0  \t0.0131516 \t0.0259633 \n",
            "73 \t114  \t0.00979866\t0  \t0.0142767 \t0.0259633 \n",
            "74 \t126  \t0.00961872\t0  \t0.0131296 \t0.0259633 \n",
            "75 \t100  \t0.0094391 \t0  \t0.0147257 \t0.0259633 \n",
            "76 \t121  \t0.00975973\t0  \t0.0110355 \t0.0259633 \n",
            "77 \t117  \t0.0116974 \t0  \t0.0156222 \t0.0261824 \n",
            "78 \t107  \t0.0112311 \t0  \t0.0175537 \t0.0261824 \n",
            "79 \t128  \t0.0105218 \t0  \t0.0121093 \t0.0261824 \n",
            "80 \t116  \t0.0101627 \t0  \t0.013042  \t0.0261824 \n",
            "81 \t137  \t0.00921291\t0  \t0.010665  \t0.0261824 \n",
            "82 \t115  \t0.00907517\t0  \t0.0120099 \t0.0261824 \n",
            "83 \t101  \t0.00906151\t0  \t0.0127841 \t0.0261824 \n",
            "84 \t127  \t0.00934666\t0  \t0.00932424\t0.0261824 \n",
            "85 \t110  \t0.00908804\t0  \t0.0116467 \t0.0261824 \n",
            "86 \t111  \t0.00914774\t0  \t0.0110605 \t0.0261824 \n",
            "87 \t123  \t0.00865374\t0  \t0.0103533 \t0.0261824 \n",
            "88 \t114  \t0.00911636\t0  \t0.0103248 \t0.0261824 \n",
            "89 \t116  \t0.00886994\t0  \t0.0107561 \t0.0261824 \n",
            "90 \t122  \t0.00915807\t0  \t0.0120962 \t0.0261824 \n",
            "91 \t105  \t0.0106973 \t0  \t0.0165011 \t0.0261824 \n",
            "92 \t125  \t0.0104367 \t0  \t0.0129743 \t0.0261824 \n",
            "93 \t114  \t0.0103763 \t0  \t0.0129164 \t0.0261824 \n",
            "94 \t107  \t0.0115662 \t0  \t0.0176523 \t0.0261824 \n",
            "95 \t120  \t0.0107722 \t0  \t0.0133663 \t0.0261824 \n",
            "96 \t99   \t0.0110038 \t0  \t0.0177848 \t0.0261824 \n",
            "97 \t128  \t0.0105009 \t0  \t0.014705  \t0.0261824 \n",
            "98 \t120  \t0.0135814 \t0  \t0.0189105 \t0.0328869 \n",
            "99 \t105  \t0.0127845 \t0  \t0.0191847 \t0.0328869 \n",
            "100\t114  \t0.0142493 \t0  \t0.0217484 \t0.0330357 \n",
            "101\t118  \t0.0138807 \t0  \t0.0164303 \t0.0330357 \n",
            "102\t113  \t0.0137621 \t0  \t0.0148738 \t0.0330357 \n",
            "103\t123  \t0.0124888 \t0  \t0.01145   \t0.0330357 \n",
            "104\t104  \t0.013002  \t0  \t0.0149621 \t0.0330357 \n",
            "105\t138  \t0.0137888 \t0  \t0.0182361 \t0.0330357 \n",
            "106\t106  \t0.012614  \t0  \t0.0185643 \t0.0330357 \n",
            "107\t124  \t0.0127013 \t0  \t0.0158525 \t0.0330357 \n",
            "108\t117  \t0.0128151 \t0  \t0.0148501 \t0.0330357 \n",
            "109\t134  \t0.0116693 \t0  \t0.0143999 \t0.0330357 \n",
            "110\t115  \t0.0116322 \t0  \t0.0147242 \t0.0330357 \n",
            "111\t130  \t0.0116248 \t0  \t0.0143646 \t0.0330357 \n",
            "112\t117  \t0.0113257 \t0  \t0.014759  \t0.0330357 \n",
            "113\t134  \t0.0111116 \t0  \t0.0142161 \t0.0330357 \n",
            "114\t117  \t0.0109092 \t0  \t0.0133065 \t0.0330357 \n",
            "115\t110  \t0.0102175 \t0  \t0.0140435 \t0.0330357 \n",
            "116\t111  \t0.0103814 \t0  \t0.0142242 \t0.0330357 \n",
            "117\t117  \t0.0111322 \t0  \t0.0158487 \t0.0330357 \n",
            "118\t114  \t0.0122125 \t0  \t0.018231  \t0.0330357 \n",
            "119\t106  \t0.0120145 \t0  \t0.0173417 \t0.0330357 \n",
            "120\t112  \t0.0124612 \t0  \t0.0190942 \t0.0330357 \n",
            "121\t120  \t0.0122619 \t0  \t0.0167493 \t0.0330357 \n",
            "122\t111  \t0.011934  \t0  \t0.0176941 \t0.0330357 \n",
            "123\t122  \t0.0124968 \t0  \t0.016842  \t0.0330357 \n",
            "124\t110  \t0.012249  \t0  \t0.0153098 \t0.0330357 \n",
            "125\t128  \t0.0113829 \t0  \t0.0118663 \t0.0330357 \n",
            "126\t123  \t0.0112836 \t0  \t0.0128802 \t0.0330357 \n",
            "127\t130  \t0.0108377 \t0  \t0.0111206 \t0.0330357 \n",
            "128\t130  \t0.0110288 \t0  \t0.0107842 \t0.0330357 \n",
            "129\t137  \t0.0110727 \t0  \t0.0127297 \t0.0330357 \n",
            "130\t119  \t0.0117079 \t0  \t0.0160931 \t0.0330357 \n",
            "131\t116  \t0.0118999 \t0  \t0.015548  \t0.0330357 \n",
            "132\t130  \t0.011375  \t0  \t0.0123918 \t0.0330357 \n",
            "133\t133  \t0.0106564 \t0  \t0.0112683 \t0.0330357 \n",
            "134\t130  \t0.0105709 \t0  \t0.0116425 \t0.0330357 \n",
            "135\t124  \t0.0102088 \t0  \t0.00869529\t0.0330357 \n",
            "136\t124  \t0.0104555 \t0  \t0.0110714 \t0.0330357 \n",
            "137\t119  \t0.0102108 \t0  \t0.0112733 \t0.0330357 \n",
            "138\t116  \t0.0103557 \t0  \t0.010967  \t0.0330357 \n",
            "139\t111  \t0.0112616 \t0  \t0.0126657 \t0.0330357 \n",
            "140\t130  \t0.010728  \t0  \t0.0125177 \t0.0330357 \n",
            "141\t115  \t0.0100605 \t0  \t0.0128347 \t0.0330357 \n",
            "142\t118  \t0.00998753\t0  \t0.0102057 \t0.0330357 \n",
            "143\t113  \t0.0103055 \t0  \t0.0111847 \t0.0330357 \n",
            "144\t124  \t0.0100011 \t0  \t0.00970168\t0.0330357 \n",
            "145\t116  \t0.0102242 \t0  \t0.0109432 \t0.0330357 \n",
            "146\t121  \t0.0107388 \t0  \t0.0110403 \t0.0330357 \n",
            "147\t104  \t0.0116683 \t0  \t0.0125088 \t0.0330357 \n",
            "148\t125  \t0.0118731 \t0  \t0.0124344 \t0.0330357 \n",
            "149\t130  \t0.0103827 \t0  \t0.00998762\t0.0330357 \n",
            "150\t114  \t0.0110455 \t0  \t0.00976933\t0.0330357 \n",
            "151\t129  \t0.0103398 \t0  \t0.0102278 \t0.0330357 \n",
            "152\t118  \t0.010627  \t0  \t0.00944393\t0.0330357 \n",
            "153\t113  \t0.00991543\t0  \t0.0103088 \t0.0330357 \n",
            "154\t120  \t0.0106227 \t0  \t0.011249  \t0.0330357 \n",
            "155\t132  \t0.00975629\t0  \t0.00893314\t0.0330357 \n",
            "156\t119  \t0.00963666\t0  \t0.0095352 \t0.0330357 \n",
            "157\t125  \t0.0106855 \t0  \t0.0108797 \t0.0330357 \n",
            "158\t121  \t0.0112932 \t0  \t0.0121087 \t0.0330357 \n",
            "159\t132  \t0.0110692 \t0  \t0.0120505 \t0.0330357 \n",
            "160\t137  \t0.0103046 \t0  \t0.0101595 \t0.0330357 \n",
            "161\t122  \t0.0110276 \t0  \t0.0123262 \t0.0330357 \n",
            "162\t125  \t0.0106605 \t0  \t0.0120501 \t0.0330357 \n",
            "163\t127  \t0.010563  \t0  \t0.0108877 \t0.0330357 \n",
            "164\t97   \t0.0106884 \t0  \t0.0117016 \t0.0330357 \n",
            "165\t121  \t0.0111875 \t0  \t0.0111362 \t0.0330357 \n",
            "166\t126  \t0.0108598 \t0  \t0.0103997 \t0.0330357 \n",
            "167\t135  \t0.0098885 \t0  \t0.00881689\t0.0330357 \n",
            "168\t124  \t0.00928676\t0  \t0.00776849\t0.0330357 \n",
            "169\t106  \t0.00933682\t0  \t0.00784947\t0.0330357 \n",
            "170\t122  \t0.00914084\t0  \t0.00832336\t0.0330357 \n",
            "171\t127  \t0.00873194\t0  \t0.00903134\t0.0330357 \n",
            "172\t141  \t0.00779462\t0  \t0.0064706 \t0.0330357 \n",
            "173\t102  \t0.00936903\t0  \t0.00872564\t0.0330357 \n",
            "174\t104  \t0.00998482\t0  \t0.00986463\t0.0330357 \n",
            "175\t123  \t0.0101597 \t0  \t0.00980859\t0.0330357 \n",
            "176\t120  \t0.00972742\t0  \t0.0093453 \t0.0330357 \n",
            "177\t121  \t0.0108754 \t0  \t0.00979734\t0.0330357 \n",
            "178\t116  \t0.0110602 \t0  \t0.00882483\t0.0330357 \n",
            "179\t116  \t0.0108992 \t0  \t0.00895364\t0.0330357 \n",
            "180\t123  \t0.0115604 \t0  \t0.0100331 \t0.0330357 \n",
            "181\t130  \t0.0108247 \t0  \t0.00966753\t0.0330357 \n",
            "182\t118  \t0.0108964 \t0  \t0.00899003\t0.0330357 \n",
            "183\t137  \t0.0100505 \t0  \t0.00823924\t0.0330357 \n",
            "184\t112  \t0.0105231 \t0  \t0.00874896\t0.0330357 \n",
            "185\t124  \t0.0109508 \t0  \t0.0094917 \t0.0330357 \n",
            "186\t123  \t0.0108151 \t0  \t0.0090247 \t0.0330357 \n",
            "187\t126  \t0.00971471\t0  \t0.00762074\t0.0330357 \n",
            "188\t122  \t0.0103993 \t0  \t0.00964114\t0.0330357 \n",
            "189\t121  \t0.0106916 \t0  \t0.0098758 \t0.0330357 \n",
            "190\t112  \t0.0111656 \t0  \t0.0114423 \t0.0330357 \n",
            "191\t130  \t0.0117893 \t0  \t0.0097898 \t0.0330357 \n",
            "192\t130  \t0.0119363 \t0  \t0.00914674\t0.0330357 \n",
            "193\t127  \t0.0104995 \t0  \t0.00828161\t0.0330357 \n",
            "194\t125  \t0.0100083 \t0  \t0.00800025\t0.0330357 \n",
            "195\t124  \t0.0105274 \t0  \t0.00801542\t0.0330357 \n",
            "196\t107  \t0.0122995 \t0  \t0.0104517 \t0.0330357 \n",
            "197\t112  \t0.013007  \t0  \t0.0112811 \t0.0330357 \n",
            "198\t117  \t0.0131807 \t0  \t0.0108596 \t0.0330357 \n",
            "199\t116  \t0.0124304 \t0  \t0.0105835 \t0.0330357 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc-wqAxoT45D"
      },
      "source": [
        "**Prediction task**\n",
        "\n",
        "1.   Create new data from the list of fittest individual (fittest features) for both training and testing data.\n",
        "2.   Fit the svm with transformed training data\n",
        "3.   Predict the transformed testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRyeLuWSrGwr"
      },
      "source": [
        "#1. Evaluate the expression for training set\n",
        "list_train_vecs = []\n",
        "max_value_fit1 = max([individual.fitness.values[0] for individual in hof])\n",
        "# max_value_fit2 = max([individual.fitness.values[1] for individual in hof])\n",
        "\n",
        "for individual in hof:\n",
        "  if individual.fitness.values[0] == max_value_fit1:\n",
        "      func = toolbox.compile(expr=individual)\n",
        "      vec = []\n",
        "      for x_train in X_train: #Iterate every vector x (row) in data (matrix) X\n",
        "        try:\n",
        "          val = func(*x_train)\n",
        "          vec.append(val)\n",
        "        except:\n",
        "          vec.append(0)\n",
        "      list_train_vecs.append(vec)\n",
        "\n",
        "#2. Evaluate the expression for testing set\n",
        "list_test_vecs = []\n",
        "for individual in hof:\n",
        "  if individual.fitness.values[0] == max_value_fit1:\n",
        "      func = toolbox.compile(expr=individual)\n",
        "      vec = []\n",
        "      for x_test in X_test: #Iterate every vector x (row) in data (matrix) X\n",
        "        try:\n",
        "          val = func(*x_test)\n",
        "          vec.append(val)\n",
        "        except:\n",
        "          vec.append(0)\n",
        "      list_test_vecs.append(vec)\n",
        "\n",
        "#3. Convert list_vecs to numpy array\n",
        "X_train_new = np.array(list_train_vecs).T   #Need to refactor X_train g\n",
        "X_train_new = np.nan_to_num(X_train_new, copy=True, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "X_train_new = np.hstack((X_train, X_train_new))\n",
        "\n",
        "X_test_new = np.array(list_test_vecs).T   #Need to refactor X_test g\n",
        "X_test_new = np.nan_to_num(X_test_new, copy=True, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "X_test_new = np.hstack((X_test, X_test_new))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_CnWmUscRHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb60f2e8-4c7d-48b1-8c02-0d696ecb0967"
      },
      "source": [
        "#Evaluation\n",
        "#1. Fit the SVM\n",
        "list_accuracy_result = []\n",
        "list_f1_result = []\n",
        "for c in range(-5,6):\n",
        "  for g in range(-4,6):\n",
        "    clf_svc = SVC(C=2**c, gamma=2**g)\n",
        "    clf_svc.fit(X_train, y_train)\n",
        "    y_pred = clf_svc.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    list_accuracy_result.append(accuracy)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    list_f1_result.append(f1)\n",
        "\n",
        "print(\"SVM - Highest Accuracy-Score (wo weight): \", max(list_accuracy_result))\n",
        "print(\"SVM - Highest F1-Score (wo weight): \", max(list_f1_result))\n",
        "\n",
        "#2. Fit the SVM\n",
        "list_accuracy_result = []\n",
        "list_f1_result = []\n",
        "for c in range(-5,6):\n",
        "  for g in range(-4,6):\n",
        "    clf_svc = SVC(C=2**c, gamma=2**g, class_weight=class_weight)\n",
        "    clf_svc.fit(X_train, y_train)\n",
        "    y_pred = clf_svc.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    list_accuracy_result.append(accuracy)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    list_f1_result.append(f1)\n",
        "\n",
        "print(\"CS-SVM - Highest Accuracy-Score: \", max(list_accuracy_result))\n",
        "print(\"CS-SVM - Highest F1-Score: \", max(list_f1_result))\n",
        "\n",
        "#3. Fit the SVM (SVGPM)\n",
        "list_accuracy_result = []\n",
        "list_f1_result = []\n",
        "for c in range(-5,6):\n",
        "  for g in range(-4,6):\n",
        "    clf_svc = SVC(C=2**c, gamma=2**g)\n",
        "    clf_svc.fit(X_train_new, y_train)\n",
        "    y_pred = clf_svc.predict(X_test_new)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    list_accuracy_result.append(accuracy)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    list_f1_result.append(f1)\n",
        "\n",
        "print(\"SVGPM - Highest Accuracy-Score: \", max(list_accuracy_result))\n",
        "print(\"SVGPM - Highest F1-Score: \", max(list_f1_result))\n",
        "\n",
        "\n",
        "#4. Fit the SVM (SVGPM)\n",
        "list_accuracy_result = []\n",
        "list_f1_result = []\n",
        "for c in range(-5,6):\n",
        "  for g in range(-4,6):\n",
        "    clf_svc = SVC(C=2**c, gamma=2**g, class_weight=class_weight)\n",
        "    clf_svc.fit(X_train_new, y_train)\n",
        "    y_pred = clf_svc.predict(X_test_new)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    list_accuracy_result.append(accuracy)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    list_f1_result.append(f1)\n",
        "\n",
        "print(\"CS-SVGPM - Highest Accuracy-Score: \", max(list_accuracy_result))\n",
        "print(\"CS-SVGPM - Highest F1-Score: \", max(list_f1_result))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM - Highest Accuracy-Score (wo weight):  0.9727272727272728\n",
            "SVM - Highest F1-Score (wo weight):  0.8695652173913043\n",
            "CS-SVM - Highest Accuracy-Score:  0.9272727272727272\n",
            "CS-SVM - Highest F1-Score:  0.6923076923076924\n",
            "SVGPM - Highest Accuracy-Score:  0.9818181818181818\n",
            "SVGPM - Highest F1-Score:  0.9166666666666666\n",
            "CS-SVGPM - Highest Accuracy-Score:  0.9454545454545454\n",
            "CS-SVGPM - Highest F1-Score:  0.7272727272727272\n"
          ]
        }
      ]
    }
  ]
}